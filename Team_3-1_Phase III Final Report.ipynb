{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbfed01-7a2f-4840-bb03-f333bd74c9ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Enhancing Airline Operations: Utilizing Decision Trees for Proactive Flight Delay Management\n",
    "\n",
    "## Team 3-1 Members (Alphabetical)\n",
    "\n",
    "[annie]: https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Team%20Photos/annie.png?raw=true\n",
    "[lily]: https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Team%20Photos/lily.png?raw=true\n",
    "[abby]: https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Team%20Photos/abby.png?raw=true\n",
    "[kevin]: https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Team%20Photos/kevin.jpg?raw=true\n",
    "\n",
    "\n",
    "| ![annie]                | ![lily]                  | ![abby]                   | ![kevin]                |\n",
    "| ----------------------- | ------------------------ | ------------------------- | ----------------------- |\n",
    "| Annie Friar             | Lily Magliente           | Abby Purnell              | Kevin Stallone          |\n",
    "| anniefriar@berkeley.edu&ensp;&ensp;&ensp;&thinsp; | magliente@berkeley.edu&ensp;&ensp;&ensp;   | abbypurnell4@berkeley.edu | kstallone@berkeley.edu&ensp;&ensp;&ensp;&thinsp;&thinsp;&thinsp; |\n",
    "\n",
    "## Phase Leaders \n",
    "| Phase             | Leader   |\n",
    "| :---------------- | :------: | \n",
    "| Phase 1: Project Plan, describe datasets, joins, tasks, and metrics|   Lily Magliente  |\n",
    "| Phase 2: EDA, baseline pipeline, Scalability, Efficiency, Distributed/parallel Training, and Scoring Pipeline           | Abby Purnell    |\n",
    "| **Phase 3: Select the optimal algorithm, fine-tune and submit a final report**    |**Annie Friar** |\n",
    "| Homework 5 | Kevin Stallone |\n",
    "\n",
    "## Credit Assignment Plan \n",
    "| Phase | Task          | Description | Estimated Time (hour) | Deadline |Member | Status |\n",
    "| ----- | ------------- | :------: | :-: | :----: | :----: | :-: |\n",
    "| 3     |  Abstract | Implement abstract feedback from previous phases | 0.5 | 4/18 | Abby Purnell | ✅ |\n",
    "| 3     |  Project Description | Update description of task at hand for current phase| 0.5 | 4/20 | Annie Friar | ✅ |\n",
    "| 3     |  Workflow Diagram | Overarching workflow diagram to provide a visual context for the general workflow | 1 | 4/21 | Kevin Stallone | ✅ |\n",
    "| 3     |  Data Description | Finalize description of the data used | 0.5 |4/18 | Abby Purnell | ✅ |\n",
    "| 3     |  EDA | Finalize data description and visualization of each of the input and target features | 13 | 4/20 | Lily Magliente | ✅ |\n",
    "| 3     |  Feature Engineering | Improve upon engineered features and provide correlation analysis for temporal variables | 25 | 4/15 | Abby Purnell | ✅ |\n",
    "| 3     |  Modeling Pipelines | Describe which algorithms we'll use, what metrics we'll base our analysis on, and our pipleline's steps | 1 | 4/19 | Annie Friar | ✅ |\n",
    "| 3     |  Dataset creation | Create various train, val, and test datasets  | 12 |4/11 | Annie Friar | ✅ |\n",
    "| 3     |  Addressing Leakage | Define and give an example of leakage  | 1 | 4/18 | Annie Friar |  ✅  |\n",
    "| 3     |  Experiments: Logistic Regression | Experiment with logistic regression models and report results | 8 |4/12 | Annie Friar |  ✅  |\n",
    "| 3     |  Experiments: Random Forest |  Experiment with random forest models and report results  | 10 |4/13 | Annie Friar |  ✅  |\n",
    "| 3     |  Experiments: Graident Boosted |  Experiment with gradient boosted models and report results | 10 |4/14 | Annie Friar |  ✅  |\n",
    "| 3     |  Experiments: Ensemble Models | Experiment with ensemble models and report results | 10 | 4/15 | Annie Friar |  ✅  |\n",
    "| 3     |  Experiments: Neural Networks |  Experiment with neural network models and report results | 35| 4/19 | Kevin Stallone | ✅ |\n",
    "| 3     |  Train Distributed Neural Networks (Extra Credit)  |  Train NN using CPU clusters in a distributed fashion | 20 | 4/20 | Kevin Stallone & Lily Magliente| ✅ |\n",
    "| 3     |  Post-Mortem Analysis| Highlight areas of strength and weakness in the models | 7 |4/15 | Abby Purnell | ✅ |\n",
    "| 3     |  Next Steps | Identify potential next steps | 0.5 | 4/20 | Annie Friar | ✅ |\n",
    "| 3     |  Conclusion | Updating conclusion from Phase 2 and providing final thoughts  | 1 | 4/20 | Kevin Stallone | ✅ |\n",
    "| 3     |  Report Format | Make sure all markdown is formatted correctly, that we have all needed elements present| 5 |4/21 | All | ✅ |\n",
    "| 3     |  2020-2022 Data (Extra Credit) | Pull the data for recent years, provide a clean dataset for flights and/or weather for years 2020-2022| 20 |4/15 | Abby Purnell  | ✅ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a427d2f-4e26-4b16-84a0-c2498a7a7817",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Gantt Phase 3 Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca96bcd-6dc5-4566-a14c-d46d62628d0c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.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\" width=\"1500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgICBnYW50dAogICAgICAgIHRpdGxlIFBoYXNlIDMgR2FudHQgVGltZWxpbmUKICAgICAgICBkYXRlRm9ybWF0ICBNTS1ERC1ZWVlZCiAgICAgICAgYXhpc0Zvcm1hdCAleAoKICAgICAgICBQaGFzZSAzIDogbWlsZXN0b25lLCBtMywgMDQtMjEtMjAyNCwgMWQKCiAgICAgICAgc2VjdGlvbiBQaGFzZSAzCiAgICAgICAgVXBkYXRlZCBNZXRhIEluZm8gICAgICAgICAgICAgICA6ZG9uZSwgMDQtMDktMjAyNCwyZAogICAgICAgIERhdGEgYW5kIEZlYXR1cmUgRW5naW5lZXJpbmcgICAgOmRvbmUsIDA0LTA5LTIwMjQsNmQKICAgICAgICBFeHRyYSBDcmVkaXQgUmVjZW50IERhdGEgICAgICAgIDpkb25lLCAwNC0wOS0yMDI0LDZkCiAgICAgICAgRmluYWxpemUgQWJzdHJhY3QgICAgICAgICAgICAgICA6ZG9uZSwgMDQtMDktMjAyNCwgMDQtMTgtMjAyNAogICAgICAgIExlYWthZ2UgICAgICAgICAgICAgICAgICAgICAgICAgOmRvbmUsIDA0LTA5LTIwMjQsOWQKICAgICAgICBNb2RlbGluZyBQaXBlbGluZXMgICAgICAgICAgICAgIDpkb25lLCAwNC0wOS0yMDI0LDEwZAogICAgICAgIE5ldXJhbCBOZXR3b3JrICAgICAgICAgICAgICAgICAgOmRvbmUsIDA0LTA5LTIwMjQsMDQtMjAtMjAyNAogICAgICAgIEZpbmFsaXplIFJlc3VsdHMgICAgICAgICAgICAgICAgOmRvbmUsIDA0LTA5LTIwMjQsMTFkCiAgICAgICAgRmluYWxpemUgRGlzY3Vzc2lvbiAgICAgICAgICAgICA6ZG9uZSwgMDQtMDktMjAyNCwxMWQKICAgICAgICBGaW5hbGl6ZSBDb25jbHVzaW9uICAgICAgICAgICAgIDpkb25lLCAwNC0wOS0yMDI0LDExZAogICAgICAgIEV4dHJhIENyZWRpdCBOZXVyYWwgTmV0d29yayAgICAgOmRvbmUsIDA0LTA5LTIwMjQsMTFkCiAgICAgICAg\" width=\"1500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def mm(graph, **kwargs):\n",
    "    \"\"\"\n",
    "    Renders a Mermaid diagram from a graph string and displays it as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - graph (str): The Mermaid graph string to be rendered.\n",
    "    - **kwargs: Additional keyword arguments to be passed to the Image constructor.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Notes:\n",
    "    Code from https://www.linkedin.com/posts/bilal-bobat_databricks-mermaid-diagram-activity-7141005243269509120-TyQD/\n",
    "\n",
    "    \"\"\"\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string, **kwargs))\n",
    "\n",
    "mm(\"\"\"\n",
    "    gantt\n",
    "        title Final Project & HW5 Gantt Working Timeline\n",
    "        dateFormat  MM-DD-YYYY\n",
    "        axisFormat %x\n",
    "\n",
    "        Phase 1 : milestone, m1, 03-17-2024, 1d\n",
    "        Phase 2 : milestone, m2, 04-07-2024, 1d\n",
    "        Phase 3 : milestone, m3, 04-21-2024, 1d\n",
    "        Homework 5 : milestone, m4, 04-14-2024, 1d\n",
    "\n",
    "        section Phase 1\n",
    "        Cluster setup                           :done, 03-9-2024, 03-12-2024 \n",
    "        Abstract                                :done, 03-12-2024, 03-16-2024\n",
    "        Data Description & EDA                  :done, 03-12-2024, 03-16-2024\n",
    "        ML Algorithms & Pipeline                :done, 03-12-2024, 03-16-2024\n",
    "        Train / Validation / Test Split         :done, 03-12-2024, 03-17-2024\n",
    "        Conclusion and Next Steps               :done, 03-12-2024, 03-17-2024\n",
    "        Project Plan                            :done, 03-12-2024, 03-17-2024\n",
    "        Report Format                           :done, 03-16-2024, 03-17-2024\n",
    "\n",
    "        section Phase 2\n",
    "        Updated Meta Info       :done, meta_info, 03-19-2024,2d\n",
    "        Updated Credit Plan     :done, 03-19-2024, 2d\n",
    "        EDA                     :done, eda2, 03-20-2024, 03-24-2024\n",
    "        Data Description        :done, 03-20-2024, 03-31-2024\n",
    "        Extra Credit            :done, after eda2, 03-31-2024\n",
    "        Presentation            :done, pres, after eda2, 04-03-2024\n",
    "        Modeling Pipelines      :done, pipelines2, after eda2, 04-05-2024\n",
    "        Results                 :done, after pres, 04-07-2024\n",
    "        Discussion              :done, after pres, 04-07-2024\n",
    "        Conclusion              :done, after pres, 04-07-2024\n",
    "\n",
    "        section Phase 3\n",
    "        Updated Meta Info               :done, 04-09-2024,2d\n",
    "        Data and Feature Engineering    :done, 04-09-2024,6d\n",
    "        Extra Credit Recent Data        :done, 04-09-2024,6d\n",
    "        Finalize Abstract               :done, 04-09-2024, 04-18-2024\n",
    "        Leakage                         :done, 04-09-2024,9d\n",
    "        Modeling Pipelines              :done, 04-09-2024,10d\n",
    "        Neural Network                  :done, 04-09-2024,04-20-2024\n",
    "        Finalize Results                :done, 04-09-2024,11d\n",
    "        Finalize Discussion             :done, 04-09-2024,11d\n",
    "        Finalize Conclusion             :done, 04-09-2024,11d\n",
    "        Extra Credit Neural Network     :done, 04-09-2024,11d\n",
    "\n",
    "        section Homework 5\n",
    "        Question 1              :done, 03-19-2024, 04-09-2024\n",
    "        Question 2              :done, 03-19-2024, 04-09-2024\n",
    "        Question 3              :done, 03-19-2024, 04-09-2024\n",
    "        Question 4              :done, 03-19-2024, 04-09-2024\n",
    "        Question 5              :done, 03-19-2024, 04-09-2024\n",
    "        Finalize Homework       :done, 04-07-2024, 04-13-2024\n",
    "        \"\"\", width=1500)\n",
    "\n",
    "mm(\"\"\"\n",
    "    gantt\n",
    "        title Phase 3 Gantt Timeline\n",
    "        dateFormat  MM-DD-YYYY\n",
    "        axisFormat %x\n",
    "\n",
    "        Phase 3 : milestone, m3, 04-21-2024, 1d\n",
    "\n",
    "        section Phase 3\n",
    "        Updated Meta Info               :done, 04-09-2024,2d\n",
    "        Data and Feature Engineering    :done, 04-09-2024,6d\n",
    "        Extra Credit Recent Data        :done, 04-09-2024,6d\n",
    "        Finalize Abstract               :done, 04-09-2024, 04-18-2024\n",
    "        Leakage                         :done, 04-09-2024,9d\n",
    "        Modeling Pipelines              :done, 04-09-2024,10d\n",
    "        Neural Network                  :done, 04-09-2024,04-20-2024\n",
    "        Finalize Results                :done, 04-09-2024,11d\n",
    "        Finalize Discussion             :done, 04-09-2024,11d\n",
    "        Finalize Conclusion             :done, 04-09-2024,11d\n",
    "        Extra Credit Neural Network     :done, 04-09-2024,11d\n",
    "        \"\"\", width=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a1f37a4-f764-41a0-812b-4ef03f0cf13c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "The International Air Transport Association predicts that 2024 will observe a significant increase in air travel for passengers (Dooley, 2023). This surge has placed pressure on airlines to forecast flight delays, a critical factor affecting airline operations. Using historical flight and weather data alongside machine learning techniques, we aim to proactively predict the binary outcome - if a *flight is delayed by more than 15 minutes or not*. We will make this prediction two hours prior to the scheduled departure time. We will market our model to airlines to empower them to proactively respond via operations, resources and adjustments of schedules based on the model’s predicted delays.\n",
    "\n",
    "We use a combined dataset from the US Department of Transportation and National Oceanic and Atmospheric Administration (NOAA) which includes flight and weather data fom 2015 to 2019. We conduct our work in Databricks for a parallelized workflow when exploring data and tuning model parameters. We use the F2 score for model evaluation, which captures the optimization of precision and recall, but places more weight on the recall metric. This metric fits our aim to minimize false negative predictions, where we predict on time but the flight is ultimately delayed. We begin with a baseline logistic regression model on a single year of data, which provides a maxmium test F2 score of 0.52. Then, using data from multiple years, we explore logistic regression, random forest, gradient boosted trees, neural networks, as well as an ensemble model which combines logistic regression and random forest predictions. The ensemble model proves to be the best model with a test F2 score of 0.58. \n",
    "\n",
    "These findings underscore the model's potential to enhance airline delay management, as well as illuminating avenues for future improvements, which could ulimately result in a customer-centric, reliable, and predictive final model for improved air travel experiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33eb2441-814e-42ff-abc2-849d62a5e2a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Project Description\n",
    "The primary objective of this project is to use machine learning models to predict flight delays. Through analyzing historical flight data alongside pertinent weather information, we seek to identify patterns and factors that contribute to flight delays. The ultimate goal is to provide airlines with actionable insights that can improve operational efficiency and enhance the overall travel experience for passengers.\n",
    "\n",
    "##### Methodology\n",
    "1. **Data Collection**: We gathered historical flight data from the Department of Transportation, encompassing a range of features including departure and arrival airports, times, airline carriers, and flight routes. We collected weather data from the National Oceanic and Atmospheric Administration (NOAA), which includes meteorological variables such as temperature, visbility, and wind speed. Additionally, we sourced supplementary data on specific airports from a public domain source, OurAirports, and weather station information also from NOAA. See \"Data Description\" below for referencs.\n",
    "\n",
    "2. **Data Preprocessing**: Once the data was collected, we cleaned and joined the sources to compile one dataset that contained flight information and weather information at the arrival and departure airport around two hours prior to departure (See \"Custom Join\" below). This also involved casting columns to the correct datatype, handling null values, feature engineering, random downsampling to address class inbalance, and feature normalization. We also performed exploratory data analysis (EDA) on the raw data sources in addition to the combined data sources to validate our understanding of the data as well as identify outliers and patterns within the data. \n",
    "\n",
    "3. **Modeling**: We explored logistic regression, random forest, gradient boosted trees, and an ensemble of logistic regression and random forest models to build a predictive model for flight delays. We split the data into training, validation, and test datasets. The final models were trained on 2016-2018 data, validated on February through June 2019 data, and tested on July through December 2019 data.  The models were then evaluated using F2 score. \n",
    "\n",
    "4. **Experimentation**: We used hyperopt to find parameter values that were the best fit for our model. We extracted those parameters and applied them back to our training and validation data, balancing finding a well fitted yet generalizable model. We also aim to decrease the number of features through regularization techniques like Lasso, keeping only the features that the regularization techniques find to be the most important.\n",
    "\n",
    "5. **Results & Next Steps**: We trained our models on the final training dataset using the identified hyperparameters, and evaluated the model against the unseen validation and test sets. This provided insight into how well our model performs on unseen data, providing a glimpse into its operational reliability.\n",
    "\n",
    "Below is a diagram roughly summarizing our workflow. The subsequent sections in this report will go into each aspect of this diagram in greater detail. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "620b3222-297c-44c8-994a-ef59b17083d8",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.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\" width=\"2500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mm(graph, **kwargs):\n",
    "    \"\"\"\n",
    "    Renders a Mermaid diagram from a graph string and displays it as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - graph (str): The Mermaid graph string to be rendered.\n",
    "    - **kwargs: Additional keyword arguments to be passed to the Image constructor.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Notes:\n",
    "    Code from https://www.linkedin.com/posts/bilal-bobat_databricks-mermaid-diagram-activity-7141005243269509120-TyQD/\n",
    "\n",
    "    \"\"\"\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string, **kwargs))\n",
    "\n",
    "mm(\"\"\"\n",
    "    flowchart LR\n",
    "        subgraph collection [Data Collection]\n",
    "        start[\"Data Collection\"] --- DoT[\"DoT\"]\n",
    "        start --- NOAA[\"NOAA\"]\n",
    "        start --- OA[\"OurAirports\"]\n",
    "        end\n",
    "        subgraph preprocess [Data Preprocessing]\n",
    "        DoT -- cleaning --> cj([\"Custom Join\"])\n",
    "        NOAA -- cleaning --> cj\n",
    "        OA -- cleaning --> cj\n",
    "        cj --> ds[\"Joined Dataset\"]\n",
    "        ds --- cp1>\"Data Checkpoint\"]\n",
    "        ds --> split[\"Train/Validation/Test Split\"]\n",
    "        split --- cp2>\"Training, Validation, and Test Sets Checkpoint\"]\n",
    "        end\n",
    "        subgraph modeling [Modeling and Experimentation]\n",
    "        cp2 --> train[\"Training Set (2016-2018)\"]\n",
    "        cp2 --> val[\"Validation Set (Feb - Jun 2019)\"]\n",
    "        cp2 --> test[\"Test Set (Jul - Dec 2019)\"]\n",
    "        train --> exp([\"Experiments\"])\n",
    "        val --> exp\n",
    "        exp -- HyperOpt --> train\n",
    "        exp --> model[\"Models\"]\n",
    "        test --> model\n",
    "        model --- cp3>\"Models Checkpoint\"]\n",
    "        model --- lr[\"Logistic Regression\"]\n",
    "        model --- nn[\"Neural Networks (MLP and Keras)\"]\n",
    "        model --- rf[\"Random Forest\"]\n",
    "        model --- gbt[\"Gradient Boosted Trees\"]\n",
    "        lr --> exp2([\"Ensemble Experiments\"])\n",
    "        rf --> exp2([\"Ensemble Experiments\"])\n",
    "        gbt --> exp2([\"Ensemble Experiments\"])\n",
    "        exp2 --> ens[\"Ensemble Model\"]\n",
    "        ens --- cp3\n",
    "        end\n",
    "        subgraph results [Results]\n",
    "        lr --> eval[\"Model Evaluation\"]\n",
    "        rf --> eval[\"Model Evaluation\"]\n",
    "        gbt --> eval[\"Model Evaluation\"]\n",
    "        ens --> eval[\"Model Evaluation\"]\n",
    "        end\n",
    "        \"\"\", width=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ff8671-8534-4c1c-995e-9e6c641f0eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Description\n",
    "\n",
    "##### Flight Data:\n",
    "  For this phase, we are utilizing [flight data](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ) from the US Department of Transportation spanning years 2015-2019. This dataset contains crucial information to describe a flight. It includes features like the flight date, airline, departure and arrival times, airtime, distance, airport details such as location. It also has binary indicators for cases of flight diversions or cancellations, as well as additional features to describe arrival and departure delays. The flights data includes breakdowns of delay durations into various categories like carrier delay and weather delay. It is important to note that in cases where flights are diverted or cancelled, columns pertaining to the flight may yield null values for features like departure time, tail number, and taxi and wheels on/off information. Conversely, when flights are not delayed, delay-related columns will be null. Given this project is focused on delayed flights, we will remove instances of cancelled or diverted flights. We explored using the following columns:\n",
    "\n",
    "| Column | Description          | Data Type | % Null Values in Joined Data\n",
    "| ----- | ------------- | :------: |:------: |\n",
    "| OP_CARRIER_AIRLINE_ID     | Unique Airline Identifier  | String | 0%\n",
    "| ORIGIN     | Departure Airport IATA Code  | String | 0%\n",
    "| DEST     | Arrival Airport IATA Code  | String | 0%\n",
    "| FL_DATE     | Date of Flight  | Date | 0%\n",
    "| DAY_OF_WEEK    | Scheduled departure day of the week | Integer | 0%\n",
    "| CRS_DEP_TIME     | Scheduled Departure Time (local time: hhmm) | Integer | 0%\n",
    "| DISTANCE     | Distance between airports in miles | Integer | 0%\n",
    "| DEP_DEL15     | Binary indicator if a flight is 15 minutes or more delayed | Integer | 0%\n",
    "\n",
    "##### Airport Data\n",
    "\n",
    " The [airport data]( http://ourairports.com/data/) is a supplementary dataset sourced from a public domain site, OurAirports, that aggregates airport information. The columns provide information regarding the airport, type, and location, which we will use to assist the join.\n",
    " \n",
    "| Column | Description          | Data Type | % Null Values in Joined Data\n",
    "| ----- | ------------- | :------: |:------: |\n",
    "| iata_code     | Unique airport identifier | String | 0%\n",
    "| gps_code     | Unique weather station identifier | Double | 0%\n",
    "\n",
    "\n",
    "##### Weather Data\n",
    " The [weather data](https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf)  is sourced from the National Oceanic and Atmospheric Administration Repository (NOAA).  The weather-related columns encompass elements such as visibility, wind, dew point, and sky cover  at stations proximate to the airport. We are limiting the weather reports to be sourced from FM-15 reports, which is the METAR Aviation routine weather report and relevant for our flight analysis. The raw data requires multiple features to be parsed out of a single column. We explored using the following columns:\n",
    "\n",
    " | Column | Description          | Data Type | % Null Values in Joined Data\n",
    "| ----- | ------------- | :------: |:------: |\n",
    "| STATION     | Unique Weather Station Identifier | String | 1.2%\n",
    "| DATE     | Date-Timestamp of weather measurement | Timestamp | 0%\n",
    "| REPORT_TYPE     | Code that denotes type of geophysical surface observation | String | 0%\n",
    "| VIS_DIST  | Visibility observation - distance at which an object can be seen (in meters) | Integer | 11.5%\n",
    "| WND_DIR  | Wind direction angle (in angular degrees) | Integer | 20.3%\n",
    "| WND_SP | Wind speed rate (in meters per second) | Integer | 1.2%\n",
    "| CIG_HGT | Height above ground level of the lowest cloud (in meters) | Integer | 19.7%\n",
    "| TMP_DEG | Temperature (in Degrees Celsius) | Integer | 0.9%\n",
    "| DEW_DEG | Dewpoint in degrees (in Degrees Celsius) | Integer | 1.7%\n",
    "\n",
    "##### Weather Station Data\n",
    "\n",
    " The [weather station data](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516) is a supplementary dataset utilized in the join also souced NOAA. The columns provide information regarding location, altitude, and the distance between stations. The following columns are utilized:\n",
    "\n",
    "| Column | Description          | Data Type | % Null Values in Joined Data\n",
    "| ----- | ------------- | :------: |:------: |\n",
    "| neighbor_id     | Unique Weather Station Identifier | String | 0%\n",
    "| neighbor_call     | Unique Airport Station Identifier | String | 0%\n",
    "\n",
    "These datasets independently provide useful information regarding flights, weather, weather stations and airports, but joining the datasets will offer a comprehensive view of the relationships between these variables. This cohesive dataset will encompass relationships between the datasets, which will ultimately advance the insights and modelled patterns.\n",
    "\n",
    "\n",
    "##### 2020 - 2022 Flight and Weather Data:\n",
    "\n",
    "Additionally, we downloaded, compiled, and cleaned the 2020 - 2022 flight and weather data sources from the US Department of Transportation and NOAA, which may be helpful for future iterations and development of the models. The flights data includes ~17.4 million records spanning 2020-2022, while the weather data includes ~388.1 million records. The final compilation of data was completed in [this notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3944613696799434/command/3944613696799435) and saved to parquet files in the following locations: *`/Workspace/Users/abbypurnell4@berkeley.edu/2020_2022_weather`* &  *`/Workspace/Users/abbypurnell4@berkeley.edu/2020_2022_flights`*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e46bc9-cb98-49ee-bc62-520ff829228a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis \n",
    "\n",
    "We performed Exploratory Data Analysis (EDA) on each of the data sources to identify outliers and validate completeness and correctness of the data. For modeling purposes, we will split our data into training, validation and test data to avoid biased evaluation of model performance and to avoid overfitting (see further details in the Machine Learning Pipeline section below). The following EDA was completed on the relevant training data.\n",
    "\n",
    "##### Flights\n",
    "The flights data source includes ~31.7 million records between 2015 and 2019. We removed instances of flights that were diverted or cancelled. The goal of the project is to predict delays, so we only wanted to include flights that successfully completed their scheduled route. We are battling extreme class imbalance in the flights data, where there are about four times as many instances of on time flights as compared to delayed flights. This will require careful consideration as we design and evaluate our models. Visualization of the imbalance can be seen below. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/flight_status_histogram.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "We might also be interested in the histogram of delays, binned by the amount of delay in minutes. We can see that the majority of delays are between 15 and 29 minutes and as the binned delay times increase, the number of delay instances decreases. The bar color shows the median departure hour. We observe through this chart that flight delays typically occur in the mid afternoon across all delay durations. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/histogram%20of%20delayed%20flights%202.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "While examining delays in relation to temporal factors, such as the day of the week, offers valuable insights, exploring the percentage of flights delayed by each airline can provide additional depth. In the following chart, we see that NK, Spirit Airlines, has the highest percent of both departing and arriving delayed flights. We also see that HA, Hawaiian Airlines, has both the lowest percentage of departing and arriving flight delays. It makes sense that the range between percentage departing flight delays and arriving flight delays is small for each airline, as a delayed flight might cause future departing and arriving delays for that airline. This interconnectedness highlights the cascading effect of delays within airline operations.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/flight_delays_dept_arr_by_airline.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "In addition to exploring the dataset's variables, it is also crucial to examine the number of nulls present to help us understand what information might be missing. For the columns with null values, there are many columns that have 100% null values. This is due to these columns only being relevant for flights that were either diverted or cancelled (which have been removed from our datasets). Since we're looking to predict the delay, we are not considering instances of cancellations or flight diversions, which in turn, yields columns that are completely null. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/Percentage%20of%20Nulls%20for%20Each%20Column%20in%20Flights%20Data.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "##### Weather Data\n",
    "The Weather dataset includes basic weather elements including wind, visibility, and temperature which are the most commonly reported, as they are mandatory reportings. The dataset also includes additional weather readings, which are not always available at specific weather stations or points of time. This leads to high null values across columns such as SLP_HPAS which measures station level pressure, or GA1_HEIGHT which measures sky condition observations. For this reason, we will be focused on the mandatory fields within the weather data.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/percentage_nulls_gauge.png?raw=true\" width=\"200\" height=\"300\">\n",
    "<br>\n",
    "\n",
    "##### Stations\n",
    " \n",
    "The Stations data does not have any null values, as seen in the table below. \n",
    "\n",
    "<br>\n",
    "\n",
    "| index                | Count of Null Values |\n",
    "|----------------------|----------------------|\n",
    "| usaf                 | 0                    |\n",
    "| wban                 | 0                    |\n",
    "| station_id           | 0                    |\n",
    "| lat                  | 0                    |\n",
    "| lon                  | 0                    |\n",
    "| neighbor_id          | 0                    |\n",
    "| neighbor_name        | 0                    |\n",
    "| neighbor_state       | 0                    |\n",
    "| neighbor_call        | 0                    |\n",
    "| neighbor_lat         | 0                    |\n",
    "| neighbor_lon         | 0                    |\n",
    "| distance_to_neighbor | 0                    |\n",
    "\n",
    "<br>\n",
    "\n",
    "We can also see through the skewed histogram below that a lot of the neighbors fall in the shorter distances and there are fewer instances of neighbors with larger distances. This is important insight to try to understand the data better. However, we are only utilizing the unique station ID and GPS code from the station data source as a supplementary source used to complete the join.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/Amount%20of%20Neighbors%20By%20Distance.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "##### Airports\n",
    "\n",
    "The raw airports data contains information on 9,225 airports and heliports across the world. The chart below calculates the percentage null values for a columns that contain null values. The iata_code has the most null values - this is because a vast majority of records are small airports that do not get IATA codes. We will only be utilizing this dataset for airports that have scheduled passenger flights, all of which have IATA codes.\n",
    "\n",
    "<br>\n",
    "\n",
    "| index        | ValueCount |\n",
    "|--------------|------------|\n",
    "| iata_code    | 0.839344   |\n",
    "| local_code   | 0.477021   |\n",
    "| gps_code     | 0.276206   |\n",
    "| elevation_ft | 0.136065   |\n",
    "| municipality | 0.102645   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a23621f-58fb-48c2-ada5-87cc442886f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Full Joined Dataset (Training Data)\n",
    "When looking at the entire training dataset, instead of excusively looking at the number of delays in the dataset, we will consider the percent of flights delayed to provide a different perspective for delay comparison. This graph exhibits that Monday has the highest percentage of flights delayed while Saturday has the lowest percentage of flights delayed. We also wanted to analyze the median hour when delays occurred. Every day except Tuesday has the a median hour delay of 1pm, where Tuesday's median hour delay is 12pm. We observed similar patterns in previous charts.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/flight_delays_by_day.png?raw=true\" width=\"500\" height=\"400\"> \n",
    "<br>\n",
    "\n",
    "The following chart reveals that the days before and after a holiday have higher percentage of delayed flights. This is expected, as the holidays are notorious for high volume of traffic in the airport because people are traveling to spend time with their families for the holidays. The high demand for flights might make delays more susceptible because resources like pilots, flight attendants, planes, and security are already stretched during these times. \n",
    "\n",
    "Unexpectedly, the days leading up to and following a holiday see lower mean delay times, as measured in minutes. However, it is notable that there is a major class imbalance in that there are more instances of No Holidays than Holidays +- 3 days. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20FP2/pie%20chart%202.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    " \n",
    "The following table provides the numerical values that describe the delayed flights three days before, on, and three days after a holiday. \n",
    " \n",
    "\n",
    "<br>\n",
    "\n",
    "| Holiday (0/1) | Number of Delays | Number of Flights | Mean Departure Delay (min) | STD. Departure Delay (min) | Percent Flights Delayed |\n",
    "|---------------|------------------|-------------------|----------------------------|----------------------------|-------------------------|\n",
    "| 0             | 630836           | 3207889           | 59.945147                  | 63.878159                  | 19.665                  |\n",
    "| 1             | 125564           | 586599            | 58.277468                  | 60.773369                  | 21.405                  |\n",
    "\n",
    "<br>\n",
    "\n",
    "The next chart investigates the visibility distance and wind speed variables. It distinguishes when there is a delay and when there is not a delay, colored by the measurements at departure and arrival airports. First and foremost, it shows that when there is a delay, the weather is typically more severe (lower visibility and higher wind speeds).\n",
    "\n",
    "Also of interest is that when there is no delay, the disparities in average weather variables between arrival and departure airports are negligible. However, when there is a delay, we see more of a difference in weather conditions between the arrival and departure airports. This points to the fact that weather conditions that are more out of the ordinary contribute to delays.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20Phase%203/Barchart%20Weather.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "We conducted a Pearson correlation analysis on important continuous variables to identify correlations with departure delay. We chose Pearson correlation as it assesses linear relationships between our variables and provides useful information for linear modeling. The highest correlated feature to departure delay (0.38) was the *average delay of a plane's tail number in the past day*. This was unsurprising, as when a plane is delayed, its next flights are susceptible to delays. The second highest correlation was the *percent of departing flights delayed per day*, at 0.23. A positive correlation (0.13) between *departure delays* and whether the *previous seven flights were delayed* was observed, suggesting a trend in consecutive delays. \n",
    "Finally, the *scheduled departure time* (0.12), *actual departure time* (0.20) and *percentage of flights delayed within the past 14 days per flight number* (0.14) all exhibited positive correlations with departure delays. We did not use actual departure time because it is information that would not be available to us two hours prior to a flight's departure. It also has significant multicollinearity with scheduled departure time.\n",
    "\n",
    "The remaining variables all had correlations of 0.08 or below when compared to departure delay and can be found in the heatmap below:\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Correlation%20Analysis/pearson_heatmap.png?raw=true\" width=\"1200\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<!-- ![Correlation Heatmap](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Correlation%20Analysis/pearson_heatmap.png?raw=true) <br> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6092afd4-9345-4309-af73-f9850d621734",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Custom Join \n",
    "\n",
    "To enhance our ability to customize the dataset and features effectively, we conducted a comprehensive join of the flight and weather tables, as detailed in our [notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2513299033247533/command/2513299033247534). The original pre-joined data provided has a few areas that need improvement. The pre-joined dataset only includes weather information at the departure airport, overlooking the potential significance of weather conditions at the arrival airport. Recognizing the importance of weather conditions at both departure and arrival points, we will improve on this by also including weather from the arrival airport. Additionally, we decided to only rely on one weather report type in order to keep measurements consistent. The weather readings included in our dataset are from FM-15 report types, which is the METAR Aviation routine weather report and relevant for our flight analysis. The visualization below displays the relational database diagram, highlighting each of the tables utilized and their join keys, which are bolded. \n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/custom_join_vis_20240404.png?raw=true\" width=\"800\">\n",
    "\n",
    "\n",
    "First, we cleaned the stations dataset, which initially comprised of 2,237 weather stations and their distances to every other station, including itself. As our focus was on station IDs, location, and call letters rather than inter-station distances, we filtered the dataset to retain only one row per station ID. Next, we performed an inner join between the stations data and the airport data using the GPS code from the airport data and the call letters from the stations data. The airport data encompasses 57,421 airports and heliports worldwide. Following the join, 1,981 airports remained with associated weather stations. Subsequently, using the timezonefinder Python package and the latitude and longitude of the weather station coordinates, we calculated the time zone of each weather station, a necessary step in order to calculate flight time in UTC for a later join.\n",
    "\n",
    "We then conducted an inner join between the combined weather station and airport data, and the flight data, utilizing the departure airport IATA code as well as the arrival IATA code. The raw flight data comprises approximately ~31.7 million flights spanning 2015-2019, ~31.2 million of those being neither delayed nor cancelled. Post-join, ~30.9 million flights remained, all associated with departure and arrival airports possessing weather stations. Notably, airports in Guam, Puerto Rico, and Samoa lacked associated weather stations. While considering the option of joining the nearest weather station to these airports, we ultimately opted to exclude them from our dataset, as using a weather station located a few miles away from the airport could potentially impact weather readings.\n",
    "\n",
    "The weather dataset consisted of ~627 million records, ~309 million of which were related to the FM-15 report type. After combining the flights and station data, we proceeded to perform left joins for the weather data, once for the departure airport and again for the arrival airport. This was based on the respective arrival and departure weather station IDs, as well as the flight and weather timestamps (in UTC). We incorporated all weather readings for both the departure and arrival airports occurring closest to the two hour mark between two and four hours of the scheduled departure time. Subsequently, we selected the closest reading to the two-hour mark prior to departure. After this join, we are left with ~30.89 million flights with weather readings at both departure and arrival airports. We checkpointed this final dataset as a parquet to be used for our pipeline. Using 8 cores, this join took 11 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5783f9ab-0aed-4528-981c-37a9323e880e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modeling Pipelines\n",
    "\n",
    "<img src = \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/261_Final_Project_ML_Pipeline%20Diagram_Phase_3.png?raw=true\"  width=\"800\" align=\"center\">\n",
    "\n",
    "##### Feature Engineering\n",
    "\n",
    "Using insights from our exploratory data analysis, we employed feature engineering techniques on the train, test, and validation datasets to enrich our dataset and try to enhance model performance. We began by introducing binary indicators to signify flight occurrences on holidays and within a three-day window around holidays. Anticipating increased flight volumes during such periods, these flags aim to capture potential associations with elevated delay probabilities.\n",
    "\n",
    "Additionally, we leveraged historical flight records to derive informative features across various time windows (1, 7, 14, and 30 days). For instance, we computed percentage of delays for specific flight numbers. We also computed percentage of delays for each given airline and origin airport, across the four time windows. To further enhance our models post-2015, we delved into the realm of rolling percentages of delays from the previous year, aiming to capture seasonal fluctuations. For a flight scheduled for departure on December 31, 2016, we computed the percentage of delayed flights in the preceding 1, 7, 14, and 30 days as of December 31, 2015, in anticipation of shedding light on potential delay patterns based on the time of year.\n",
    "\n",
    "We then calculated the Pearson's correlation of the various temporal variables against the delay length on the 2015 training data, shown below. Notably, for the previous year feature, we utilized Pearson's correlations from the 2016 training data. We moved forward with those variable windows that had the highest correlation when compared to the other windows of the same feature. These include the percent of flights of the same route in the previous 14 days that were delayed, the percent of flights at the departure airport that were delayed the day prior to the scheduled departure, the percent of flights of the airline that were delayed the day prior to departure, and the percent of flights delayed 30 days prior to the scheduled departure in the previous year.  \n",
    "\n",
    "<br>\n",
    "<img src = \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Correlation%20Analysis/2015_pearson_correlation.png?raw=true\"  width=\"700\" align = 'left'>\n",
    "<img src = \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Correlation%20Analysis/2016_pearson_correlation.png?raw=true\"  width=\"700\" align = 'right'>\n",
    "\n",
    "Furthermore, we calculated the percent of flights that had been delayed out of the departure airport as of two hours prior to scheduled departure. We assume if there have been a large number of delays at a given airport on that day, there may be a higher liklihood of cascading delays in that airport, on that day. Similarly, we calculated the average delay length for the specific aircraft tail number as of two hours prior to departure. We hope this feature will capture delays that were due to a delay from the incoming flight. We also calculated the average delay duration for both the departure airport and carrier airline over the previous month. These metrics provide valuable context for understanding historical delay patterns and their potential recurrence.\n",
    "\n",
    "Once we selected these additional features, we performed further EDA to understand patterns and trends. We used boxplots to exhibit the distributions of the temporal variables represented as percentages. These variables, described above, are as follows with their naming schema, for reference when analyzing the graphs:\n",
    "\n",
    "- Pct_prev_14_flights_delayed (Percent of flights of the same route in the previous 14 days that were delayed)\n",
    "- Airline_pct_prev_1_days_delay (Percent of flights of that airline that were delayed the day prior to the scheduled departure)\n",
    "- Origin_airport_day_pct_delay (Percent of flights at the departure airport that were delayed the day prior to the scheduled departure)\n",
    "- Rolling_30_day_prev_year_pct_flight_delays (Percent of flights delayed for the rolling previous 30 days in the previous year, to capture seasonality)\n",
    "\n",
    "Pct_prev_14_flights_delayed has a wide distribution between the first and third quantile with outliers above 50%, which indicates considerable variability in the percentage of flights delayed in the previous 14 days. Both Airline_pct_prev_1_days_delay and origin_airport_day_pct_delay do not have quite as wide of a range between the first and third quartile but again, has a lot of outlier values just above 35%. Rolling_30_day_prev_year_pct_flight_delays has a smaller distribution of values and the outliers do not extend far beyond the 3rd quartile. This indicates a more consistent pattern of flight delays over this period of time. These plots display varying degrees of varaibility and outlier presence among different variables as they pertain to flight delays. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20Phase%203/Boxplots%20of%20Percentage%20Variables.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "In the chart below, we are further assessing the same temporal variables as percentages but grouping them by carrier. We summed the percentage values for each carrier, and while they aren't percentages that will sum to a whole, they do provide insight into the reliability or delay frequency of each carrier. United Airlines and Hawaiian Airways have the smallest summed values, while budget airlines like Jet Blue and Frontier have the highest values. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20Phase%203/weather%20variables%20summed.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "We also created a histogram of the daily average delay tail number. The count values are scaled by log to better illustrate the counts on the graph. There are equally a lot of values in the -100 and 0 bins and as our bin values increase, the population in each bin decreases. The distribution suggests there is a substantial number of aircrafts that have very small daily delays, no delays, or even arrive ahead of schedule. \n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Phase%201/EDA%20Images/EDA%20Phase%203/Histogram%20of%20Delay%20Tail%20Number%20Average%20by%20Day.png?raw=true\" width=\"500\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "\n",
    "##### Building Feature Sets\n",
    "\n",
    "Once all the features were calculated, we imputed null values for columns with less than 10% null values, using medians based on the training data. Additionally, we applied normalization to numeric features using a MinMaxScaler, again based on the training data. We chose a MinMaxScaler rather than a Standard Scaler in order to preserve the distribution of the underlying features, which was not normal in all cases. This scaling made it possible for our models to learn more easily, especially in the case of logistic regression. For all our experiment datasets we checkpointed the train, validation, and test datasets to parquet at multiple points throughout the feature engineering process to avoid duplication of work and runtime. \n",
    "\n",
    "We utilized cross validation throughout each year of our training, which required careful consideration to avoid leakage between training, validation, and test datasets, where leakage is defined as leveraging data in the model that would not be available at the time of prediction. For every unique cross validation fold we were careful to scale and impute based on the training set for that fold, not extracting information from other folds, to avoid leakage. We were mindful of the importance of training on a full year of flight data as delays are impacted by seasonality in travel. With that in mind, we trained different models for each year, and validated on the first quarter of the following year. For example, we trained the 2016 model on the entirety on 2016, and validated on January through March of 2017. All final models were trained on data spanning 2016 - 2018, validated on data data Febuary 2019 - June 2019, and tested on data from July 2019 through December 2019.  \n",
    "\n",
    "Additionally,  we created four folds per year within our 2016 - 2018 data in order to explore optimizing the hyperparameters. For each year, we used HyperOpt to calculate the optimal hyperparameters within each fold, and then used the average of those to train the model. In the case of the Logistic Regression and Random Forest models, we created roughly balanced folds with 7 months of training data, validating using the next two months of data. In the case of Gradient Boosted trees, we modified this slightly to add an additional validation set that was used to determine when the model could benefit from early stopping, to avoid overfitting. This is discussed in greater detail in the results section.\n",
    "\n",
    "##### Addressing Class Imbalance\n",
    "\n",
    "The dataset exhibits significant class imbalance, with the majority of flights being categorized as not delayed, where there were about four times the amount of on time flights as compare to delayed. To address this imbalance and prevent the model from being biased towards the majority class, we applied *random downsampling* to the training datasets. This technique requires randomly selecting a subset of non-delayed flights to match the number of delayed flights, creating a balanced training set with an equal representation of both classes. By doing so, we aim to improve the model's ability to learn from the minority class instances effectively, leading to more accurate predictions and better performance on unseen data.\n",
    "\n",
    "##### Addressing Leakage\n",
    "\n",
    "Working with time series data, it is critical to determine if there has been any data leakage during the modeling process. Leakage can lead to over-inflated results, where the model is benefitting from knowledge of future data that it will not have once applied in a production setting. We took care to avoid leakage at all stages of our modeling efforts. During feature engineering, we identified only features that would be known two hours or more before flight departure time. During our feature scaling and imputation, we were careful to scale and impute based on training data alone. This was true for all our datasets, including the cross validation folds we built. Using the summary statistics of the validation or test dataset to impute or scale values would be an example of leakage, because we would be using information that would not be available to us at the time of inference (two hours prior to the flight's scheduled departure time). Finally, we used a validation set to compare our models’ performance along the way, and did not evaluate our test set until the very end when our model selection decisions were already made. \n",
    "\n",
    "##### Feature Families\n",
    "\n",
    "We used lasso regularization to select the features we utlimately used within our model. \n",
    "We ran experiments with multiple input feature families to test their performance. A feature family refers to a categorical feature, previously represented as a single column in the data, but transformed into a sparse matrix. The first, raw flight features, consists of features related to the flight itself, including the day of the week, month, carrier, departure and arrival airport, scheduled departure time, and flight distance. The categorical variables (all but the departure time and distance) were one hot encoded. However, these one hot encoded features were not ultimately used in our final models. From there, we added a family of raw weather variables, including the visibility and wind speed at both the arrival and departure airport. Then, we added a family of the engineered features, which include those discussed in the feature engineering section above. \n",
    "\n",
    "##### Machine Learning Algorithms & Loss Functions\n",
    "\n",
    "In our modeling task we explored the use of a variety of machine learning techniques, including logistic regression, neural networks, random forest, and gradient boosted trees. We discuss our use of each technique in greater detail in the results section. Here, we share the loss functions that underlie each technique.\n",
    "\n",
    "$$\\textit{Logistic Regression Model (Log Loss): }L_{log}(y,p) = -\\left(y\\log(p) + (1-y)\\log(1-p)\\right)$$\n",
    "\n",
    "$$\\textit{Neural Network Model (Log Loss): }L_{log}(y,p) = -\\left(y\\log(p) + (1-y)\\log(1-p)\\right)$$\n",
    "\n",
    "$$\\textit{Random Forest Model (Gini Impurity): }\\sum_{k != k'} \\hat{P}_{mk} \\hat{P}_{m k'} = \\sum_{k=1}^{K} \\hat{P}_{mk}(1 - \\hat{P}_{mk})$$\n",
    "\n",
    "$$\\textit{Gradient Boosted Trees Model (Log Loss): }L_{log}(y,p) = -\\left(y\\log(p) + (1-y)\\log(1-p)\\right)$$\n",
    "\n",
    "##### Evaluation Metric\n",
    "\n",
    "We approached this prediction task as a binary classification problem, with the goal of predicting if a flight will be delayed by 15 minutes or more, 2 hours prior to scheduled departure. For our baseline model, we used logistic regression with a logistic loss function, binary cross entropy loss. \n",
    "\n",
    "Given our focus on proactively predicting flight delays to optimize customer support, we prioritize predicting more false positive flights (i.e., predicting a flight will be delayed when it is not) over false negatives (i.e., predicting a flight will not be delayed when it actually is). This strategy allows us to effectively allocate resources in the event of a delay. Therefore, we will place more importance on recall than precision, which emphasizes the importance of identifying all positive instances of delay without missing any.\n",
    "\n",
    "\\\\(Recall =  \\frac{True Positive \\thinspace }{True Positive \\thinspace  +  \\thinspace False Negative} \\\\)\n",
    "\n",
    "When optimizing our model's performance on this classification task, our primary metric of focus will be F2 score, which strikes the balance of optimizing precision and recall, but places more weight on the recall metric. That will allow us to achieve an optimum balance between being proactive about possible delays, while also keeping in mind avoiding too many false positives to avoid spending resources when ultimately they are not needed.\n",
    "\n",
    "\\\\(F2 \\thinspace Score = {(1+2^2)} * \\frac{Precision \\thinspace  *  \\thinspace Recall}{(2^2 * Precision) \\thinspace  +  \\thinspace Recall} \\\\)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "114d860b-9eec-45e1-afbb-49d9a8a89610",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "For our experimentation, we trained our baseline model on a variety of feature sets and evaluated the results against our key metric, F2 Score. As part of our model pipelines, we used Hyperopt, a Python package that provides algorithms for optimizing a hyperparameter search space. We began our experimentation with only airline features, and iteratively added additional features to the model, before reducing the feature set using a pure Lasso logistic regression model for dimensionality reduction. Then we experimented with classic machine learning model types, such as Random Forest and Gradient Boosted Trees, as well as experimenting with ensembling. Our baseline experiments were conducted on a cluster which autoscales 2-10 workers at 4 cores each, with a driver that has 8 cores. Our final experiments were conducted on a cluster which autoscales 2-12 workers at 8 cores each, with a driver that has 16 cores.\n",
    "\n",
    "##### Summary of Key Experiments:\n",
    "\n",
    "| Experiment | Raw Features Count | Wall Time | Train F2 Score | Val F2 Score | Test F2 Score |\n",
    "| ----- | ---- | :------: |:------: |:------: |:------: |\n",
    "| Initial 1 Year Baseline | 12 | 4m 31s | 0.64 | 0.53 | --- |\n",
    "| Logistic Regression | 13 | 10m 49s | 0.66 | 0.60 | 0.56 |\n",
    "| 2015 Quick MLP | 12 | 25m 30s | 0.67 | 0.54 | --- |\n",
    "| Random Forest | 13 | 14m 39s | 0.65 | 0.59 | 0.56 |\n",
    "| Gradient Boosted Trees | 13 | 13m 54s | 0.73 | 0.61 | 0.57 |\n",
    "| Ensemble Model | 13 | 28m 58s | 0.70 | 0.61 | 0.58 |\n",
    "\n",
    "##### Model Experiments Summary\n",
    "\n",
    "As a team we were able to experiment with a variety of model types and hyperparameter combinations. In the end, when working with the multi-year dataset (2015-2018), we were able to improve upon the results from our initial 12 month baseline experiment. Our F2 score for our best model was 0.58, which was an improvement upon the score of 0.53 from our baseline.\n",
    "\n",
    "In the end, many of our models performed similarly, resulting in only minor changes in F2 score. This is in part because we used the same feature set across all of our final models. These results led us to conclude that we may have been hitting a ceiling in terms of model performance given the data and features we employed. We will discuss future opportunities for improvement in the post mortem section. Below, we go into the specifics of our modeling efforts by model type.\n",
    "\n",
    "##### Logistic Regression\n",
    "\n",
    "For our logistic regression models, we conducted hyperparameter optimization using Hyperopt. We used four cross validation folds to determine the best values for both the regularization penalty, as well as the elastic net parameter, which determines the balance between ridge vs. lasso regularization. The coefficients were selected based on maximizing the Area Under the Precision Recall Curve (AUPRC). We averaged the best values for each fold and applied those in our final model. \n",
    "\n",
    "Our best linear regression model was tested on data from 2016-2018. It took 10 minutes and 49 seconds to optimize and train. Our model parameters were a regParam=0.01 and an elasticNetParam=0.83. The model resulted in a Train F2 Score of 0.66, a Validation F2 Score of 0.60, and a Test F2 Score of 0.56.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Multi_Year_LR_CM.png?raw=True\" width=\"400\" height=\"320\">\n",
    "<br>\n",
    "\n",
    "The model suffered from some overfitting, as evidenced by the higher performance for the training set as opposed to the test set. Below is a graph of the absolute values of the model coefficients.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/LR_Feature_Coefficients.png?raw=True\" width=\"600\" height=\"700\">\n",
    "<br>\n",
    "\n",
    "As we can see from the chart, by far the most important feature was the Average Tail Number Delay. This shows the value of our feature engineering efforts to uncover that insight. It is clear that previous delays on the same airline and route is highly predictive of future delay.\n",
    "\n",
    "##### Neural Networks (MLLib MLP)\n",
    "\n",
    "In addition to our Logistic Regression models, we trained and ran experiments on nine neural networks––eight Multilayer Perceptron Classifiers (MLP) using MLlib and one Keras Sequential model using Horovod to distribute the compute onto multiple CPU clusters. In a similar fashion to the logistic regression models, we used four cross validation folds and averaged the hyperparamers from each fold to create our final models.\n",
    "\n",
    "###### Multilayer Perceptron Classifiers Models\n",
    "\n",
    "As previously mentioned, we created eight MLP models––conducting cross validation over two models for each of the four years. Optimizing hyperparamters is a critical yet timely step in model creation, so we then created two models per year to compare both their optimization results and their compute times. The first four models for 2015 through 2018 prioritized a shorter optimization time and thus are named \"quick.\" Each of the models took around four minutes per fold to optimize using HyperOpt, totaling about sixteen minutes per model. Then, the training phase took between five to ten minutes for each model. In total, it took roughly an hour and a half to produce these models. We prioritized a longer optimization time for the second set of models. Each model took approximately fifty minutes per fold to optimize and an additional five to ten minutes to train, totaling about fourteen compute hours for completion. The network architecture for the eight MLP models can be found in the table below:\n",
    "\n",
    "| Model      | Architecture                                                 |\n",
    "| ---------- | ------------------------------------------------------------ |\n",
    "| 2015 Quick | InputLayer(12) - DenseLayer(4, activation='sigmoid') - DenseLayer(8, activation='sigmoid') - DenseLayer(64, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2015       | InputLayer(12) - DenseLayer(16, activation='sigmoid') - DenseLayer(4, activation='sigmoid') - DenseLayer(64, activation='sigmoid') - DenseLayer(64, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2016 Quick | InputLayer(13) - DenseLayer(4, activation='sigmoid') - DenseLayer(8, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2016       | InputLayer(13) - DenseLayer(64, activation='sigmoid') - DenseLayer(64, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2017 Quick | InputLayer(13) - DenseLayer(4, activation='sigmoid') - DenseLayer(8, activation='sigmoid') - DenseLayer(16, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2017       | InputLayer(13) - DenseLayer(64, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - DenseLayer(2, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2018 Quick | InputLayer(13) - DenseLayer(4, activation='sigmoid') - DenseLayer(8, activation='sigmoid') - DenseLayer(8, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "| 2018       | InputLayer(13) - DenseLayer(16, activation='sigmoid') - DenseLayer(4, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - DenseLayer(32, activation='sigmoid') - OutputLayer(2, activation='softmax') |\n",
    "\n",
    "As for the loss curves for our MLP models, we have provided the curves for our best performing MLP and our worst performing MLP which are 2015 quick and 2018 quick respectively:\n",
    "\n",
    "![2015 Quick Loss Curve](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Neural%20Networks/Loss%20-%202015%20Short%20Optimization.png?raw=true)\n",
    "\n",
    "![2018 Quick Loss Curve](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Neural%20Networks/Loss%20-%202018%20Short%20Optimization.png?raw=true)\n",
    "\n",
    "###### Distributed Keras Deep Learning with Horovod\n",
    "\n",
    "For our single Keras model, we went with the framework known as Horovod to take advantage of distributing the training jobs on to multiple Spark clusters.  With Keras, we implemented a simple Sequential model, optimized using an Adadelta algorithm as suggested in the Horovod documentation, and used Keras's categorical cross-entropy class (log loss) to calculate our loss. We trained the model using our entire 2015 downsampled training data and then validate using our 2015 validation data to maintain the simplicity while applying the model. The training using Horovod's Keras estimator took approximately thirty minutes which was timely in comparison to prior models. Our Keras model's architecture can be found below:\n",
    "\n",
    "| Model      | Architecture                                                 |\n",
    "| ---------- | ------------------------------------------------------------ |\n",
    "| 2015 Keras | InputLayer(12) - DenseLayer(32, activation='relu') - DenseLayer(64, activation='relu') - OutputLayer(2, activation='softmax') |\n",
    "\n",
    "Below is the loss curve for our 2015 Keras model:\n",
    "\n",
    "![2015 Keras Loss Curve](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Neural%20Networks/Loss%20-%202015%20Keras%20Horovod.png?raw=true)\n",
    "\n",
    "###### Neural Networks Results\n",
    "\n",
    "The performance of all of our neural network models can be found in the table below:\n",
    "\n",
    "| Model      | Raw Features Count | Wall Time  | Train F2 Score | Val F2 Score                   | Test F2 Score | Hyper-parameters                                             |\n",
    "| ---------- | ------------------ | ---------- | -------------- | ------------------------------ | ------------- | ------------------------------------------------------------ |\n",
    "| 2015 Quick | 12                 | 25m 30s    | 0.67           | 0.54                           | ---           | blockSize: 224<br />Layers: [12, 4, 8, 64, 2]<br />Solver: l-bfgs<br />stepSize: 0.078 |\n",
    "| 2015       | 12                 | 3h 24m 47s | 0.66           | 0.53                           | ---           | blockSize: 120<br />Layers: [12, 16, 4, 64, 64, 2]<br />Solver: l-bfgs<br />stepSize: 0.108 |\n",
    "| 2016 Quick | 13                 | 23m 23s    | 0.51           | 0.19                           | ---           | blockSize: 192<br />Layers: [13, 64, 64, 32, 32, 2]<br />Solver: gd<br />stepSize: 0.041 |\n",
    "| 2016       | 13                 | 3h 50m 32s | 0.40           | 0.39                           | ---           | blockSize: 112<br />Layers: [12, 4, 8, 64, 2]<br />Solver: l-bfgs<br />stepSize: 0.135 |\n",
    "| 2017 Quick | 13                 | 17m 20s    | 0.46           | 0.35                           | ---           | blockSize: 144<br />Layers: [13, 4, 8, 16, 2]<br />Solver: l-bfgs<br />stepSize: 0.052 |\n",
    "| 2017       | 13                 | 2h 44m 29s | 0.54           | 0.44                           | ---           | blockSize: 120<br />Layers: [13, 64, 32, 32, 2, 2]<br />Solver: gd<br />stepSize: 0.047 |\n",
    "| 2018 Quick | 13                 | 23m 50s    | 0              | 0                              | ---           | blockSize: 224<br />Layers: [13, 4, 8, 8, 2]<br />Solver: gd<br />stepSize: 0.013 |\n",
    "| 2018       | 13                 | 4h 13m 15s | 0.66           | 0.53                           | ---           | blockSize: 152<br />Layers: [13, 16, 4, 32, 32, 2]<br />Solver: l-bfgs<br />stepSize: 0.064 |\n",
    "| 2015 Keras | 12                 | 1h 36m 22s    | 0.71           | 0.57                           | ---           | Num_proc: 6<br />batch_size: 500<br />epochs: 250               |\n",
    "\n",
    "Our neural networks analysis focused on balancing time invested in optimization and model performance. Across the four years, the results showed that allowing more time for optimization yielded mixed results. For example, 2015 showed that longer optimization periods resulted in decreased F2 performance for training and validation sets for MLP, as compared to the quick model. However, this trend reversed, where models with longer optimization times consistently outperformed the quick models in 2018.\n",
    "\n",
    "In 2018, the longer optimization time yielded a model that could learn to predict \"delayed\" in some instances, whereas the quick model oversimplified and always predicted flights to be on time. The Keras model had our best F2 performance for the training data when it came to our neural network models; however, we still found that it was overfitting when we ran it against our validation data. Below are the confusion matrices for our 2015 quick and 2018 quick models as they were our best and worst MLP performers. We have also included our 2015 Keras matrix here as well as it was a different architecture than the rest:\n",
    "\n",
    "![2015 Quick Confusion Matrix](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Neural%20Networks/2015%20Model%20short%20Confusion%20matrix,%20with%20normalization.png?raw=true)\n",
    "\n",
    "![2018 Quick Confusion Matrix](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Neural%20Networks/2018%20Model%20short%20Confusion%20matrix,%20with%20normalization.png?raw=true)\n",
    "\n",
    "![2015 Keras Confusion Matrix](https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Neural%20Networks/2015%20Keras%20Confusion%20matrix,%20with%20normalization.png?raw=true)\n",
    "\n",
    "Generally speaking, these results show that time needs to be invested into optimizing these models, which is not necessarily the best trade-off when it comes to overall cost for this project. Because our neural network models were not showing much of an improvement over our other less compute intensive models, we decided not to pursue neural network models further. We therefore did not train nor validate them on the entire dataset nor did we stack the yearly MLP models to create one ensemble model from them. We felt as if it would not be a good investment of resources to do so.\n",
    "\n",
    "##### Random Forest\n",
    "\n",
    "For our random forest models, we conducted hyperparameter optimization using Hyperopt. We used four cross validation folds to determine the best values for the max tree depth, the number of trees, the minimum weight per node, and the subsampling rate. The coefficients were selected based on maximizing the Area Under the Precision Recall Curve (AUPRC). We averaged the best values for each fold and applied those in our final model. \n",
    "\n",
    "Our best random forest model was tested on data from 2016-2018. It took 14 minutes and 39 seconds to optimize and train. Our model parameters were a maxDepth=3, numTrees=41, minWeightFractionPerNode=0.01, and subsamplingRate=0.77. The model resulted in a Train F2 Score of 0.65, a Validation F2 Score of 0.59, and a Test F2 Score of 0.56.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Multi_Year_RF_CM.png?raw=True\" width=\"400\" height=\"320\">\n",
    "<br>\n",
    "\n",
    "The model suffered from some overfitting, as evidenced by the higher performance for the training set as opposed to the test set. Below is a graph of the feature importance.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/RF_Feature_Importance.png?raw=True\" width=\"600\" height=\"500\">\n",
    "<br>\n",
    "\n",
    "Again, Average Tail Number Delay was the most predictive feature. The random forest model, however, focused on other features in a more balanced way, such as Origin Airport % Delay and Previous 14 Days Flights Delayed. This could indicate that it was able to pick up on non-linear relationships between these features and whether or not a flight would be delayed.\n",
    "\n",
    "##### Gradient Boosted Trees\n",
    "\n",
    "For these models, we again conducted hyperparameter optimization using Hyperopt. In the case of gradient boosted trees, we built a special cross validation strategy. Gradient boosted trees can suffer from overfitting, and benefit from early stopping. In order to conduct early stopping with the GBT library in MLlib, we needed a validation portion of the training set. The algorithm then, at each new iteration, tests to see if the improvement at the last step was large enough to continue iterating. To facilitate this, we used the last month of the training portion of the cross validation fold for validation during training, and then proceeded to validate the performance of the hyperparameters against another unseen validation set. In the case of all validation sets for every fold, we made sure to impute and scale our features based on the pure training dataset alone.\n",
    "\n",
    "We used our four cross validation folds to determine the best values for the max tree depth, the minimum weight per node, the subsampling rate, and the validation tolerance (which determined when the early stopping would occur). The coefficients were selected based on maximizing the Area Under the Precision Recall Curve (AUPRC). We averaged the best values for each fold and applied those in our final model. \n",
    "\n",
    "Our best gradient boosted trees model was tested on data from 2016-2018. It took 13 minutes and 54 seconds to optimize and train. Our model parameters were a maxDepth=4, minWeightFractionPerNode=0.03, subsamplingRate=0.51, and validationTol=0.03. The model resulted in a Train F2 Score of 0.73, a Validation F2 Score of 0.61, and a Test F2 Score of 0.57.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Multi_Year_CM.png?raw=True\" width=\"400\" height=\"320\">\n",
    "<br>\n",
    "\n",
    "The model suffered from some overfitting, as evidenced by the higher performance for the training set as opposed to the test set.\n",
    "\n",
    "##### Ensemble Model\n",
    "\n",
    "Given that all our models differed in their predictions, and they all suffered from some overfitting, we wanted to explore ensembling them together to see if it would result in more balanced predictions.\n",
    "\n",
    "To do so, we loaded all our trained models which had been logged to MLLib. These included models that had been trained on 12 months of data at a time, as well as the models that were trained on the 2016-2018 dataset as described above. Then, we experimented with weighting the predictions of each model differently to reach a final prediction. We explored using different weights both based on training data year, and on model type.\n",
    "\n",
    "In the end, of all the scenarios we tested, we saw the best performance when we ensembled the logistic regression model and random forest model together, both of which had been trained on the 2016-2018 dataset. The method of ensembling was to collect the predictions from each model, and if either model predicted a delay, the ensemble would predict a delay.\n",
    "\n",
    "We report that the time to train the ensemble model was 42 minutes and 52 seconds. This includes the time it takes to train each model (logistic regression and random forest), as well as the time it takes to aggregate their predictions and apply them to the unseen test dataset.\n",
    "\n",
    "<br>\n",
    "<img src= \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/Ensemble_CM.png?raw=True\" width=\"400\" height=\"320\">\n",
    "<br>\n",
    "\n",
    "The ensemble model ended up being our best performing model in terms of F2 score. The model resulted in a Train F2 Score of 0.70, a Validation F2 Score of 0.61, and a Test F2 Score of 0.58. The ensemble model is still overfit. However, it is our best performing model, and when looking at the test results, does a pretty nice job of predicting flights that will be delayed, while avoiding predicting too many false positives.\n",
    "\n",
    "In the end, all our models come with significant limitations in terms of their usefulness in practice. That will be discussed further in our post mortem section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128913c6-062b-4a80-a23a-a701a8d66108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Post Mortem Analysis & Next Steps\n",
    "\n",
    "Following the completion of our model development and testing phase, we conducted a post-mortem analysis with the goal of highlighting areas of opportunity for future model enhancements. We specifically focused on false negative predictions, and those flights that were delayed but were not identified by our models. The pie chart below shows a breakdown of false negative predictions by delay type for the best-performing ensemble model.\n",
    "\n",
    "<br>\n",
    "<img src = \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/ensemble_model_predictions.png?raw=true\"  width=\"500\" align = 'right'>\n",
    "\n",
    "The majority of flights that our models failed to identify as delayed were delayed due to either carrier delays (42.1%) or other delays (25.7%). Carrier-related delays stemmed from various factors within the airline's control, such as unexpected maintenance, strikes, baggage mishaps, catering issues, or crew delays. However, as depicted in the bar chart below, late aircraft emerged as the primary cause of flight delays in the test dataset. Impressively, our model demonstrated exceptional performance in predicting these delays, achieving an accuracy rate of 88% for late aircraft incidents compared to just 47% for carrier-related delays.\n",
    "\n",
    "Moving forward, we aim to enhance our models by collaborating with airlines to leverage additional features, including crew schedules, maintenance logs, and aircraft sensor data. Additionally, we seek to delve deeper into other unlabeled delays to uncover common patterns and characteristics that could enhance our predictive capabilities.\n",
    "\n",
    "Interestingly, despite incorporating weather features into our models, we could only predict 64% of flights affected by weather delays. However, upon applying Lasso regularization, some weather features were excluded from our selection process. Ultimately, the final models incorporated visibility and wind speed data from both departure and arrival airports. Looking ahead, we plan to expand our model testing to include more weather features, aiming to enhance our ability to predict weather-related delays.\n",
    "\n",
    "<br>\n",
    "<img src = \"https://github.com/UC-Berkeley-I-School/mids-261-team-3-1-spring24/blob/main/ensemble_model_predictions_bars.png?raw=true\"  width=\"700\" align = 'right'>\n",
    "<br>\n",
    "\n",
    "Additionally, our findings reveal a notable decrease in our F2 metric when transitioning from training to test data, suggesting the presence of overfitting. We hypothesize that our models might be capturing excessive complexity within the training data. To address this issue, we aim to streamline our features in future iterations to develop a more balanced model that generalizes better to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ec5a38-240b-4f95-b720-56c6c2021557",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Our project addresses the significant issue of flight delays in the United States, with the US Department of Transport and the Bureau of Transportation Statistics estimating that 20% of flights daily are delayed, highlighting a need for proactive flight delay management system. \n",
    "\n",
    "Our goal has been to equip airline carriers with predictive tools to enhance the overall customer experience. We hypothesized that our machine learning pipelines can proactively forecast flight delays of fifteen minutes or more up to two hours in advance. To achieve this, we developed Logistic Regression, Multilayer Perception Classification, Random Forest, Gradient Boosted Trees, and ensembled models to predict these delays.\n",
    "\n",
    "The results from our final models show some promise in proactively forecasting flight delays, particularly for late aircraft delays, but they are less successful in predicting carrier or other delays. Our best-performing model in terms of the F2 score is the ensemble model, which is a combination of our logistic regression model and our random forest model. However, it still suffers from overfitting. We found that the most important features for our models were _Average Tail Number Delay_ and _Origin Airport Percent Delay_.  In the future, we plan to address the issue of overfitting by simplifying some features to create a more balanced model. Of note, the simpler logistic regression model is fairly close in terms of performance to the ensemble and we recommend that, depending on the deployment and usage scenario, it might be more efficient to opt for the logistic regression model due to its lower computational cost.\n",
    "\n",
    "Our project represents a significant step towards proactive delay management in the aviation industry. We aim to continue refining our models, integrating more features, and delving deeper into the causes of various delays to further enhance our predictive capabilities. In the end, we made progress towards the goal of predicting and mitigating flight delays which benefits airlines, passengers, and the industry as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9daff50-b5c7-4673-acb0-efe44bb2fc16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebooks\n",
    "\n",
    "#### Phase 3\n",
    "\n",
    "1. [Custom Join](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2513299033247533/command/2513299033247536) \n",
    "2. [Exploratory Data Analysis](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2513299033247692/command/1093733074368588)\n",
    "3. [Exploratory Data Analysis Phase 3](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717732027/command/2926108717732413) \n",
    "4. [Interactive Geo Heatmap](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1093733074368792/command/1093733074368793)\n",
    "5. [Correlation Analysis](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4252851009356275/command/2769031167785018)\n",
    "6. [2015 - 2019 Data Cleaning & Feature Engineering](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3544923356434666/command/3544923356434667)\n",
    "7. [Create Multi-Year Feature Sets](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717727030/command/2926108717727031)\n",
    "8. [2016-2018 Logistic Regression Model](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717731806/command/2926108717731807)\n",
    "9. [2015-2018 Multilayer Perceptron Classifier](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3544923356433836/command/2926108717728828)\n",
    "10. [2015 Horovod Keras Model](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2340268507074316/command/2340268507074338)\n",
    "11. [2016-2018 Random Forest Model](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717731951/command/2926108717731952)\n",
    "12. [2016-2018 Gradient Boosted Trees Model with Early Stopping](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717727897/command/2926108717727898)\n",
    "13. [Multi-Year Models Evaluation](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717730022/command/2926108717730023)\n",
    "14. [Ensemble Model & Evaluation](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717726497/command/2926108717726498)\n",
    "15. [Select Additional Experiment Results](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3544923356437229/command/3544923356437245)\n",
    "16. [2020-2022 Flights & Weather Data](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3944613696799434/command/3944613696799435)\n",
    "17. [Post-Mortem Analysis](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2926108717732098/command/2926108717732099) \n",
    "\n",
    "#### Earlier Phases\n",
    "\n",
    "1. [2015 Data Cleaning & Feature Engineering](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2513299033251754/command/1835145152852867)\n",
    "2. [Building Features Sets for Baseline Models 1-3](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3944613696786448)\n",
    "3. [Building Features Sets for Baseline Models 4-6](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3944613696786875/command/3944613696786876)\n",
    "4. [Baseline Model, Run Experiments](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3944613696788174/command/3944613696788175)\n",
    "5. [Baseline Model, Collect Experiments Results](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/2769031167780340/command/2769031167780341)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2a87281-f736-4c3b-9c6e-d4c354a85bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## References\n",
    "\n",
    "1. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285355/\n",
    "2. https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html\n",
    "3. https://spark.apache.org/docs/1.5.2/mllib-ensembles\n",
    "4. https://www.databricks.com/blog/pattern-lightweight-deployment-distributed-xgboost-and-lightgbm-models\n",
    "5. https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "6. https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/\n",
    "7. https://medium.com/@hwangdb/smote-implementation-in-pyspark-76ec4ffa2f1d\n",
    "8. [Dooley, Roger. \"Survey Predicts Air Travel Boom For 2024: What It Means For Passengers.\" *Forbes, 6 Dec 2023*](https://www.forbes.com/sites/rogerdooley/2023/12/06/air-travel-boom-predicted-for-2024/)\n",
    "9. https://beverly-wang0005.medium.com/roc-vs-prc-6cd30d287a4b"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3295227484018084,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Team_3-1_Phase III Final Report",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
